---
title: "p8105_hw6_ns3782"
author: "NSK"
date: "2024-11-23"
output: github_document
---

```{r setup, include=FALSE}

knitr::opts_chunk$set(echo = TRUE)

library(tidyverse)
library(modelr)
library(ggplot2)
library(purrr)
library(mgcv)
library('rnoaa')
library(broom)

```

# Problem 1: Bootstrapping


```{r}

# Load weather data

weather_df = 
  rnoaa::meteo_pull_monitors(
    c("USW00094728"),
    var = c("PRCP", "TMIN", "TMAX"), 
    date_min = "2017-01-01",
    date_max = "2017-12-31") %>%
  mutate(
    name = recode(id, USW00094728 = "CentralPark_NY"),
    tmin = tmin / 10,
    tmax = tmax / 10) %>%
  select(name, id, everything())

# Set seed for reproducibility

set.seed(1)

# Obtain R squared values and log product of coefficient estimates from each bootstrap sample

bootstrap_results = 
  weather_df %>%
  modelr::bootstrap(n = 5000) %>%
  mutate(
    models = map(strap, \(df) lm(tmax ~ tmin, data = df)),
    r2 = map_dbl(models, \(model) glance(model)$r.squared),
    results = map(models, broom::tidy)) %>%
  unnest(results) %>%
  select(.id, term, estimate, r2) %>%
  pivot_wider(
    names_from = term,
    values_from = estimate
  ) %>%
  mutate(
    log_prod = log(`(Intercept)` * tmin)
  ) %>%
  select(r2, log_prod)

# Visualisations of estimates

bootstrap_results %>%
  pivot_longer(
    everything(),
    names_to = "statistic", 
    values_to = "value"
  ) %>%
  ggplot(aes(x = value)) +
  geom_density() +
  facet_grid(~statistic, scales = "free")

```

Based on the plot, the R-squared values peak around 0.92, which indicates a consistently strong relationship between minimum and maximum temperatures across the bootstrap samples. The distribution has a narrow range from about 0.88 to 0.94, indicating that across bootstrap samples, the model consistently explains a high proportion of the variance in maximum temperatures using minimum temperatures as a predictor. Since R-squared is bounded between 0 and 1 (as it represents the proportion of variance explained by the model), the range of values being closer to 1 indicates the model fits the data very well.

The log product of coefficients peaks around 2.0, with values ranging approximately from 1.95 to 2.10. The distribution of both estimates is bell-shaped/approximately normal, which is typical of repeated sampling. The relatively narrow, symmetric spreads indicates that these estimates are stable across bootstrap samples. This stability suggests that if new weather data were collected, we would likely find similar values for both the model fit and coefficient relationship.

The observed difference in distributions represents differences in the properties of the estimates obtained from bootstrapping: R-squared values must fall between 0 and 1 as they represent a proportion of explained variance, while the log product can exceed these bounds. Overall, this analysis shows the value of bootstrapping in understanding the sampling variability of model estimates, especially for quantities like the log product of coefficients where the theoretical sampling distribution isn't readily available.


# Problem 2: U.S. City Homicides

```{r}

# Load and clean homicide data

homicide_data <- 
  read_csv("homicide-data.csv") %>%
  mutate(
    city_state = paste(city, state, sep = ", "),
    solved = case_when(
      disposition == "Closed by arrest" ~ 1,
      disposition == "Closed without arrest" ~ 1,
      disposition == "Open/No arrest" ~ 0
    ),
    victim_age = as.numeric(victim_age)
  ) %>%
  filter(
    !(city_state %in% c("Dallas, TX", "Phoenix, AZ", "Kansas City, MO", "Tulsa, AL")),
    victim_race %in% c("White", "Black")
  )

```

## Adjusted odds ratios for solved homicides by victim sex in Baltimore, MD

```{r}

# Create new dataset filtering for Baltimore

baltimore_df <- 
  homicide_data %>%
  filter(city_state == "Baltimore, MD")

# Fit logistic regression

baltimore_solved_logistic <- 
  baltimore_df %>%
  glm(solved ~ victim_age + victim_race + victim_sex, 
      data = ., 
      family = binomial())

# Model results with estimate and confidence interval

baltimore_solved_logistic %>%
  broom::tidy(conf.int = TRUE) %>%
  mutate(
    OR = exp(estimate),
    OR_lower = exp(conf.low),
    OR_upper = exp(conf.high)
  ) %>%
  select(term, log_OR = estimate, OR, OR_lower, OR_upper, p.value) %>%
  knitr::kable(digits = 3)

```

Based on the logistic regression analysis of Baltimore homicides, the odds of solving a case are 0.35 times lower for male victims compared to female victims (OR = 0.35, 95% CI: 0.27, 0.47), after adjusting for victim age and race.

## Adjusted odds ratios for solved homicides by victim sex in 47 U.S. cities

```{r}

city_results <- homicide_data %>%
  group_by(city_state) %>%           
  nest() %>%                         
  mutate(
    results = map(data, ~glm(solved ~ victim_age + victim_race + victim_sex, 
                            data = ., family = binomial()) %>%
                    broom::tidy(conf.int = TRUE) %>%
                    filter(term == "victim_sexMale"))
  ) %>%
  unnest(results) %>%
  mutate(
    OR = exp(estimate),
    OR_lower = exp(conf.low),
    OR_upper = exp(conf.high)
  ) %>%
  select(city_state, OR, OR_lower, OR_upper, p.value)

```


```{r}

city_results %>%
  mutate(city_state = fct_reorder(city_state, OR)) %>%
  ggplot(aes(x = OR, y = city_state)) +
  geom_point() +
  geom_errorbarh(aes(xmin = OR_lower, xmax = OR_upper), height = 0.2) +
  labs(title = "Association Between Victim Sex and Solved Homicides by City",
       x = "Odds Ratio", 
       y = "City") 

```

Based on the plot, in most cities cases with male homicide victims appear to have lower odds of being solved, compared to cases with female homicide victims. This is because most of the odds ratios observed are less than 1. Of these cities, San Bernandino, Albuquerque, Durham, Richmond, Savannah, Tampa, San Francisco, Boston, Nashville, Tulsa, Birmingham and Oklahoma have confidence intervals that cross 1, indicating that for these cities, the odds ratio difference between male and female victims is not statistically significant.

Only Stockton, Minneapolis, Fresno have odds ratios greater than 1, however they are not significant based on the confidence intervals observed which include 1.



# Problem 3: Children's Birthweight


```{r}

# Load data and clean variables

birthweight <- 
  read_csv("birthweight.csv") %>%
  mutate(
    babysex = factor(babysex, levels = c(1, 2), labels = c("Male", "Female")),
    malform = factor(malform, levels = c(0, 1), labels = c("Absent", "Present")),
    frace = factor(frace, 
                  levels = c(1, 2, 3, 4, 8, 9),
                  labels = c("White", "Black", "Asian", "Puerto Rican", "Other", "Unknown")),
    mrace = factor(mrace,
                  levels = c(1, 2, 3, 4, 8),
                  labels = c("White", "Black", "Asian", "Puerto Rican", "Other"))
  )

# Check for missing values

colSums(is.na(birthweight))

```

## Backwards Selection: Regression model for birthweight

Following data cleaning, I proceeded with backwards selection to produce an appropriate regression model for birthweight.

```{r}

# Full model: including all variables in birthweight dataset

full_model <- lm(bwt ~ babysex + bhead + blength + delwt + fincome + 
                   frace + gaweeks + menarche + mheight + momage + 
                   mrace + parity + ppwt + smoken + wtgain, 
                 data = birthweight)

# Simple backwards selection

final_model <- step(full_model)

# Results of backwards selection

summary(final_model)

```

After considering the dataset variables, several were excluded from the initial model. These included variables pnumlbw (previous number of low birth weight babies) and pnumgsa (number of prior small gestational age babies) as they had very few cases, which I assumed would make them weak predictors. In addition, I chose to remove malform (presence of malformations) as this represented rare birth events that I assumed would be consequences, rather than predictors of birth weight. The decision was also made to remove ppbmi (post partum BMI) as it would likely show high correlation with other maternal weight measurements (ppwt) already included in the model, which I assumed would potentially cause multicollinearity issues.

I then proceeded with backwards selection to produce my final model. The final model contained all remaining variables based on AIC criterion, including family income and Asian race category which were not statistically significant at the 0.05 level, but were still included as they contributed to the model's overall fit. The model explains approximately 72% of the variation in birth weight (RÂ² = 0.7181).

## Residual Plot for Birthweight Regression Model

```{r}

birthweight %>%
  add_predictions(final_model) %>%
  add_residuals(final_model) %>%
  ggplot(aes(x = pred, y = resid)) +
  geom_point(alpha = 0.3) +
  geom_smooth(method = "lm") +
  geom_hline(yintercept = 0, color = "red", linetype = "dashed") +
  labs(x = "Fitted Values",
       y = "Residuals",
       title = "Residuals vs Fitted Values") 

```

The residual plot produced shows some degree of heteroscedasticity given the that the spread of residuals appears to increase as fitted values increase (fan shape). Although most of the residuals appear to cluster around zero, there are some observable outliers, especially for lower predicted birth weights.


## Model Comparison: Main model vs Length at birth + Gestational Model

After producing a final model through backwards selection, I compared its prediction accuracy against two alternative models:

1. A simple model using only length at birth and gestational age
2. An interaction model using head circumference, length, sex, and all their interactions


```{r}

# Create cross-validation splits

cv_splits = 
  crossv_mc(birthweight, 100) %>% 
  mutate(
    train = map(train, as_tibble),
    test = map(test, as_tibble)
  )

```

To compare the prediction accuracy of the three models, I first created 100 training and testing datasets using cross-validation.

```{r}

# Fit models and extract RMSEs

cv_res_df =
  cv_splits %>% 
  mutate(
    main_mod = map(train, ~lm(bwt ~ babysex + bhead + blength + delwt + fincome + 
                               frace + gaweeks + menarche + mheight + momage + 
                               mrace + parity + ppwt + smoken + wtgain, data = .x)),
    length_gest_mod = map(train, ~lm(bwt ~ blength + gaweeks, data = .x)),
    interaction_mod = map(train, ~lm(bwt ~ bhead * blength * babysex, data = .x))
  ) %>% 
  mutate(
    rmse_main = map2_dbl(main_mod, test, rmse),
    rmse_length_gest = map2_dbl(length_gest_mod, test, rmse),
    rmse_interaction = map2_dbl(interaction_mod, test, rmse)
  )


```

I then fit three models on each training dataset:

* My main model selected through backwards selection
* A simpler model using only length at birth and gestational age as predictors
* An interaction model using head circumference, length, sex, and all interactions between these variables

## RMSE (Root Mean Square Error) plot from cross-validation

```{r}

# Plot RMSE distribution

cv_res_df %>% 
  select(starts_with("rmse")) %>% 
  pivot_longer(
    everything(),
    names_to = "model", 
    values_to = "rmse",
    names_prefix = "rmse_"
  ) %>% 
  ggplot(aes(x = model, y = rmse)) + 
  geom_violin()

```

Based on the cross-validation results, the original final model from backwards selection showed the best prediction accuracy with the lowest RMSE around 270-275 grams. The interaction model using head circumference, length, sex, and their interactions performed reasonably well but had slightly higher prediction error. The simplest model using only length and gestational age had notably worse performance, with RMSE values around 330-340 grams.

This seems to suggest adding variables such as mother's characteristics provides better predictions compared to either the interaction-only or simple length-gestational age models.
