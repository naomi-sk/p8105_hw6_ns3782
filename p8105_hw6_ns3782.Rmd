---
title: "p8105_hw6_ns3782"
author: "NSK"
date: "2024-11-23"
output: github_document
---

```{r setup, include=FALSE}

knitr::opts_chunk$set(echo = TRUE)

library(tidyverse)
library(modelr)
library(ggplot2)
library(purrr)
library(mgcv)

```

# Problem 1


```{r}

# Load weather data

```

# Problem 2: U.S. City Homicides

```{r}

# Load and clean homicide data

homicide_data <- 
  read_csv("homicide-data.csv") %>%
  mutate(
    city_state = paste(city, state, sep = ", "),
    solved = case_when(
      disposition == "Closed by arrest" ~ 1,
      disposition == "Closed without arrest" ~ 1,
      disposition == "Open/No arrest" ~ 0
    ),
    victim_age = as.numeric(victim_age)
  ) %>%
  filter(
    !(city_state %in% c("Dallas, TX", "Phoenix, AZ", "Kansas City, MO", "Tulsa, AL")),
    victim_race %in% c("White", "Black")
  )

```

## Adjusted odds ratios for solved homicides by victim sex in Baltimore, MD

```{r}

# Create new dataset filtering for Baltimore

baltimore_df <- 
  homicide_data %>%
  filter(city_state == "Baltimore, MD")

# Fit logistic regression

baltimore_solved_logistic <- 
  baltimore_df %>%
  glm(solved ~ victim_age + victim_race + victim_sex, 
      data = ., 
      family = binomial())

# Model results with estimate and confidence interval

baltimore_solved_logistic %>%
  broom::tidy(conf.int = TRUE) %>%
  mutate(
    OR = exp(estimate),
    OR_lower = exp(conf.low),
    OR_upper = exp(conf.high)
  ) %>%
  select(term, log_OR = estimate, OR, OR_lower, OR_upper, p.value) %>%
  knitr::kable(digits = 3)

```

Based on the logistic regression analysis of Baltimore homicides, the odds of solving a case are 0.35 times lower for male victims compared to female victims (OR = 0.35, 95% CI: 0.27, 0.47), after adjusting for victim age and race.

## Adjusted odds ratios for solved homicides by victim sex in 47 U.S. cities

```{r}

city_results <- homicide_data %>%
  group_by(city_state) %>%           
  nest() %>%                         
  mutate(
    results = map(data, ~glm(solved ~ victim_age + victim_race + victim_sex, 
                            data = ., family = binomial()) %>%
                    broom::tidy(conf.int = TRUE) %>%
                    filter(term == "victim_sexMale"))
  ) %>%
  unnest(results) %>%
  mutate(
    OR = exp(estimate),
    OR_lower = exp(conf.low),
    OR_upper = exp(conf.high)
  ) %>%
  select(city_state, OR, OR_lower, OR_upper, p.value)

```


```{r}

city_results %>%
  mutate(city_state = fct_reorder(city_state, OR)) %>%
  ggplot(aes(x = OR, y = city_state)) +
  geom_point() +
  geom_errorbarh(aes(xmin = OR_lower, xmax = OR_upper), height = 0.2) +
  labs(title = "Association Between Victim Sex and Solved Homicides by City",
       x = "Odds Ratio", 
       y = "City") 

```

Based on the plot, in most cities cases with male homicide victims appear to have lower odds of being solved, compared to cases with female homicide victims. This is because most of the odds ratios observed are less than 1. Of these cities, San Bernandino, Albuquerque, Durham, Richmond, Savannah, Tampa, San Francisco, Boston, Nashville, Tulsa, Birmingham and Oklahoma have confidence intervals that cross 1, indicating that for these cities, the odds ratio difference between male and female victims is not statistically significant.

Only Stockton, Minneapolis, Fresno have odds ratios greater than 1, however they are not significant based on the confidence intervals observed which include 1.



# Problem 3: Children's Birthweight


```{r}

# Load data and clean variables

birthweight <- 
  read_csv("birthweight.csv") %>%
  mutate(
    babysex = factor(babysex, levels = c(1, 2), labels = c("Male", "Female")),
    malform = factor(malform, levels = c(0, 1), labels = c("Absent", "Present")),
    frace = factor(frace, 
                  levels = c(1, 2, 3, 4, 8, 9),
                  labels = c("White", "Black", "Asian", "Puerto Rican", "Other", "Unknown")),
    mrace = factor(mrace,
                  levels = c(1, 2, 3, 4, 8),
                  labels = c("White", "Black", "Asian", "Puerto Rican", "Other"))
  )

# Check for missing values

colSums(is.na(birthweight))

```

## Backwards Selection: Regression model for birthweight

Following data cleaning, I proceeded with backwards selection to produce an appropriate regression model for birthweight.

```{r}

# Full model: including all variables in birthweight dataset

full_model <- lm(bwt ~ babysex + bhead + blength + delwt + fincome + 
                   frace + gaweeks + menarche + mheight + momage + 
                   mrace + parity + ppwt + smoken + wtgain, 
                 data = birthweight)

# Simple backwards selection

final_model <- step(full_model)

# Results of backwards selection

summary(final_model)

```

After considering the dataset variables, several were excluded from the initial model. These included variables pnumlbw (previous number of low birth weight babies) and pnumgsa (number of prior small gestational age babies) as they had very few cases, which I assumed would make them weak predictors. In addition, I chose to remove malform (presence of malformations) as this represented rare birth events that I assumed would be consequences, rather than predictors of birth weight. The decision was also made to remove ppbmi (post partum BMI) as it would likely show high correlation with other maternal weight measurements (ppwt) already included in the model, which I assumed would potentially cause multicollinearity issues.

I then proceeded with backwards selection to produce my final model. The final model contained all remaining variables based on AIC criterion, including family income and Asian race category which were not statistically significant at the 0.05 level, but were still included as they contributed to the model's overall fit. The model explains approximately 72% of the variation in birth weight (RÂ² = 0.7181).

## Residual Plot for Birthweight Regression Model

```{r}

birthweight %>%
  add_predictions(final_model) %>%
  add_residuals(final_model) %>%
  ggplot(aes(x = pred, y = resid)) +
  geom_point(alpha = 0.3) +
  geom_smooth(method = "lm") +
  geom_hline(yintercept = 0, color = "red", linetype = "dashed") +
  labs(x = "Fitted Values",
       y = "Residuals",
       title = "Residuals vs Fitted Values") 

```

The residual plot produced shows some degree of heteroscedasticity given the that the spread of residuals appears to increase as fitted values increase (fan shape). Although most of the residuals appear to cluster around zero, there are some observable outliers, especially for lower predicted birth weights.


## Model Comparison: Main model vs Length at birth + Gestational Model

After producing a final model through backwards selection, I compared its prediction accuracy against two alternative models:

1. A simple model using only length at birth and gestational age
2. An interaction model using head circumference, length, sex, and all their interactions


```{r}

# Create cross-validation splits

cv_splits = 
  crossv_mc(birthweight, 100) %>% 
  mutate(
    train = map(train, as_tibble),
    test = map(test, as_tibble)
  )

```

To compare the prediction accuracy of the three models, I first created 100 training and testing datasets using cross-validation.

```{r}

# Fit models and extract RMSEs

cv_res_df =
  cv_splits %>% 
  mutate(
    main_mod = map(train, ~lm(bwt ~ babysex + bhead + blength + delwt + fincome + 
                               frace + gaweeks + menarche + mheight + momage + 
                               mrace + parity + ppwt + smoken + wtgain, data = .x)),
    length_gest_mod = map(train, ~lm(bwt ~ blength + gaweeks, data = .x)),
    interaction_mod = map(train, ~lm(bwt ~ bhead * blength * babysex, data = .x))
  ) %>% 
  mutate(
    rmse_main = map2_dbl(main_mod, test, rmse),
    rmse_length_gest = map2_dbl(length_gest_mod, test, rmse),
    rmse_interaction = map2_dbl(interaction_mod, test, rmse)
  )


```

I then fit three models on each training dataset:

* My main model selected through backwards selection
* A simpler model using only length at birth and gestational age as predictors
* An interaction model using head circumference, length, sex, and all interactions between these variables
